{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vYcB3e4zBFfi"
   },
   "outputs": [],
   "source": [
    "# Problem Statement 1\n",
    "\n",
    "wall = [(1,1)]\n",
    "terminal_states = ((1,3),(2,3))\n",
    "\n",
    "# # non - deterministic action (equally probable)\n",
    "# action_probability = {'L':0.25,'R':0.25,'U':0.25,'D':0.25}\n",
    "\n",
    "# environment action corresponding to Agent if it does not follow the desired direction (i.e follow perpendicular direction to desired one)\n",
    "environment_left = {'L':'D','R':'U','U':'L','D':'R'}\n",
    "environment_right = {'L':'U','R':'D','U':'R','D':'L'}\n",
    "\n",
    "#check validity of the cell\n",
    "def is_valid(i,j):\n",
    "    return (i,j) not in wall and i >= 0 and i < 3 and j >= 0 and j < 4\n",
    "\n",
    "#print matrix after convergence \n",
    "def print_values(V):\n",
    "  for i in range(2,-1,-1):\n",
    "    print(\" \")\n",
    "    for j in range(4):\n",
    "      v = V[i][j]\n",
    "      print(\" %.2f|\" % v, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "#take action\n",
    "def transition(action,i,j):\n",
    "    if action == 'L':\n",
    "        return (i,j-1)\n",
    "    elif action == 'R':\n",
    "        return (i,j+1)\n",
    "    elif action == 'U':\n",
    "        return (i+1,j)\n",
    "    elif action == 'D':\n",
    "        return (i-1,j)   \n",
    "    else:\n",
    "        return (-1,-1)\n",
    "\n",
    "def value_function(i,j,reward,reward_matrix,discount_factor=1):\n",
    "    value = 0\n",
    "    for action in ['L','R','U','D']:\n",
    "        # desired action with 0.8 probability\n",
    "        state_x,state_y = transition(action,i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            desired_action_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            desired_action_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
    "        \n",
    "        # environment action with 0.1 probability\n",
    "        state_x,state_y = transition(environment_left[action],i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            env_action_left_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            env_action_left_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
    "        \n",
    "        # environment action with 0.1 probability \n",
    "        state_x,state_y = transition(environment_right[action],i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            env_action_right_value = (reward_matrix[state_x][state_y] + discount_factor*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            env_action_right_value = (reward_matrix[i][j] + discount_factor*V_pie[i][j])\n",
    "        \n",
    "        value_to_action = desired_action_value*0.8+env_action_left_value*0.1+env_action_right_value*0.1        \n",
    "\n",
    "        value += value_to_action*0.25 # # non - deterministic action (equally probable)\n",
    "\n",
    "    return value\n",
    "\n",
    "# iterative policy evaluation\n",
    "def iterative_policy_evaluation(iter,epsilon,reward,reward_matrix,V_pie):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                state = (i,j)\n",
    "                if state in terminal_states or state in wall:  # continue if encounter terminal state or wall\n",
    "                    continue\n",
    "                v = V_pie[i][j]\n",
    "                V_pie[i][j] = value_function(i,j,reward,reward_matrix)\n",
    "                delta = max(delta,abs(v-V_pie[i][j]))\n",
    "        iter += 1\n",
    "        if delta < epsilon:\n",
    "            print(f\"Number of iterations to converge = {iter}\")\n",
    "            break \n",
    "    print_values(V_pie)\n",
    "\n",
    "# initialize the reward matrix with given reward value except the terminal states\n",
    "def update_reward_matrix(reward):\n",
    "    reward_matrix = [[reward for _ in range(4)] for _ in range(3)]\n",
    "    reward_matrix[2][3] = 1\n",
    "    reward_matrix[1][3] = -1\n",
    "    return reward_matrix\n",
    "\n",
    "# initialize V_pie with all zeroes at start\n",
    "def initialize_V_pie():\n",
    "    V_pie = [[0 for _ in range(4)]for _ in range(3)]\n",
    "    return V_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZVuZE5gEKau",
    "outputId": "9ae1c482-9e81-4cc5-ee0b-920a149f37d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Functions corresponding to optimal policy\n",
      "\n",
      "For r(S) : -0.04\n",
      "Number of iterations to converge = 312\n",
      " \n",
      " -1.23| -0.83| -0.28| 0.00|\n",
      " \n",
      " -1.47| 0.00| -0.87| 0.00|\n",
      " \n",
      " -1.55| -1.47| -1.22| -1.17|\n",
      "\n",
      "\n",
      "For r(S) : -2\n",
      "Number of iterations to converge = 384\n",
      " \n",
      " -59.71| -46.01| -24.32| 0.00|\n",
      " \n",
      " -65.41| 0.00| -21.94| 0.00|\n",
      " \n",
      " -63.10| -52.80| -34.49| -20.75|\n",
      "\n",
      "\n",
      "For r(S) : 0.1\n",
      "Number of iterations to converge = 324\n",
      " \n",
      " 2.95| 2.39| 1.44| 0.00|\n",
      " \n",
      " 3.10| 0.00| 0.63| 0.00|\n",
      " \n",
      " 2.85| 2.20| 1.15| 0.23|\n",
      "\n",
      "\n",
      "For r(S) : 0.02\n",
      "Number of iterations to converge = 284\n",
      " \n",
      " 0.56| 0.55| 0.46| 0.00|\n",
      " \n",
      " 0.49| 0.00| -0.23| 0.00|\n",
      " \n",
      " 0.34| 0.11| -0.20| -0.57|\n",
      "\n",
      "\n",
      "For r(S) : 1\n",
      "Number of iterations to converge = 370\n",
      " \n",
      " 29.80| 23.14| 12.48| 0.00|\n",
      " \n",
      " 32.46| 0.00| 10.30| 0.00|\n",
      " \n",
      " 31.11| 25.77| 16.43| 9.22|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rewards = [-0.04,-2,0.1,0.02,1]\n",
    "epsilon = 1e-8\n",
    "print(\"Value Functions corresponding to optimal policy\\n\")\n",
    "for reward in rewards:\n",
    "    print(f\"For r(S) : {reward}\")\n",
    "    reward_matrix = update_reward_matrix(reward)\n",
    "    V_pie = initialize_V_pie()\n",
    "    iterative_policy_evaluation(0,epsilon,reward,reward_matrix,V_pie)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2EnJWZJXGrI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
